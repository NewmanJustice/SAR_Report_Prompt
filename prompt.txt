Specification and Requirements (SAR) Document: Web Application & API Code Review

Version: 2.0
Last Updated: June 24, 2025
Document Type: LLM Code Review Specification
Scope: Web Applications and APIs

Document Purpose

This specification defines the requirements for conducting comprehensive code reviews of web applications and APIs using Large Language Model assistance. The review must analyze both frontend and backend components when present, focusing on code quality, security, performance, and maintainability according to industry best practices.

Input Requirements

Repository Analysis Scope
- Analyze the complete repository structure provided
- Review both frontend and backend code components when present
- Focus on web application frameworks, API implementations, and related infrastructure code
- Include configuration files, build scripts, and deployment specifications

Review Requirements

1. Code Quality Assessment

Complexity Analysis
Apply industry-standard thresholds:
- Functions with cyclomatic complexity > 10: Flag as medium priority
- Functions with cyclomatic complexity > 15: Flag as high priority
- Nested loops/conditionals > 3 levels: Requires refactoring consideration
- Function length > 50 lines: Review for single responsibility violations

Code Duplication Detection
- Code duplication > 5%: Requires immediate attention
- Duplicated logic blocks > 10 lines: Flag for extraction
- Copy-paste patterns: Identify and recommend consolidation

Readability Evaluation
Assess against web development standards:
- Consistent naming conventions (camelCase for JavaScript, kebab-case for CSS)
- Clear variable and function names that convey purpose
- Appropriate code organization and module structure
- Proper commenting for complex business logic
- Adherence to established style guides (ESLint, Prettier, etc.)

2. SOLID Principles Evaluation

Specific Evaluation Criteria:
- Single Responsibility: Count reasons for class/module changes, flag components with >3 responsibilities
- Open-Closed: Identify modification points, suggest extension mechanisms for new features
- Liskov Substitution: Test behavioral compatibility in inheritance hierarchies
- Interface Segregation: Review API contracts and component interfaces for specificity
- Dependency Inversion: Assess abstraction usage and dependency injection patterns

Scoring System:
Rate each principle 1-10 with specific justification and improvement recommendations.

3. Testing Evaluation

Coverage Requirements (Industry Standards):
- Minimum line coverage: 80% for critical business logic
- Branch coverage: 70% minimum for conditional logic
- Function coverage: 90% for public APIs and exported functions

Test Quality Assessment:
- Unit test presence: Core business logic must have unit tests
- Integration test coverage: API endpoints and database interactions
- End-to-end test strategy: Critical user journeys for web applications
- Test maintainability: Clear, focused test cases with descriptive names
- Mock usage: Appropriate isolation of external dependencies

Missing Test Areas:
Identify specific components lacking adequate test coverage with priority ratings.

4. Security Assessment

OWASP Top 10 Compliance (Web Applications):
- A01 Broken Access Control: Authentication and authorization implementation
- A02 Cryptographic Failures: Data encryption and secure transmission
- A03 Injection: SQL injection, XSS, and command injection vulnerabilities
- A04 Insecure Design: Security-by-design principles
- A05 Security Misconfiguration: Server and application configuration
- A06 Vulnerable Components: Dependency vulnerability scanning
- A07 Authentication Failures: Session management and user authentication
- A08 Software Integrity Failures: Supply chain security
- A09 Logging Failures: Security monitoring and incident response
- A10 Server-Side Request Forgery: SSRF prevention measures

API-Specific Security:
- Input validation: Comprehensive sanitization and validation
- Rate limiting: DoS protection implementation
- CORS configuration: Appropriate cross-origin policies
- API authentication: JWT, OAuth, or API key implementation
- Secrets management: Environment variables and secure storage

5. Performance Considerations

Web Application Performance:
- Bundle size analysis: JavaScript/CSS bundle optimization
- Loading performance: Lazy loading, code splitting implementation
- Caching strategies: Browser caching, CDN usage, API caching
- Database query optimization: N+1 queries, indexing strategies
- Memory usage patterns: Memory leaks and garbage collection

API Performance:
- Response time optimization: Database queries, external API calls
- Pagination implementation: Large dataset handling
- Async processing: Background jobs and queue management
- Resource utilization: CPU and memory efficiency

6. Documentation Review

Code Documentation:
- API documentation: OpenAPI/Swagger specifications for APIs
- Component documentation: JSDoc for JavaScript, inline comments
- README completeness: Setup, deployment, and usage instructions
- Architecture documentation: System design and data flow diagrams

Missing Documentation Areas:
Identify gaps in technical documentation with priority levels.

7. Legacy IT Risk Assessment Framework
- The LLM must score the repository against the criteria detailed below. The final risk score is calculated by multiplying the **Likelihood Score** by the **Impact Score**.

### 7.1. Scoring Instructions
- **Inference and Assumptions:** For criteria that cannot be directly measured from the code (e.g., business impact, physical environment), the LLM must make educated inferences based on available information (such as README files, documentation, library age, and issue discussions).
- **Transparency:** The final report must include a dedicated section explaining all assumptions, inferences, and interpretations made during the risk assessment. Any criteria that are omitted as non-applicable must be listed with a justification.

### 7.2. Likelihood Criteria (L)
The LLM will assess the likelihood of failure or compromise by scoring the repository against these factors. The highest score from any single criterion determines the overall Likelihood Score.

*   **L5 - Very High:**
    *   Known unpatched vulnerabilities with public exploits.
    *   Unsupported hardware or software components.
    *   Unsuitable physical environment for the system.
    *   Lack of access to necessary expertise for support.
*   **L4 - High:**
    *   Components are out of support or extended support within 12 months.
    *   Significant performance or capacity issues.
    *   Lack of regular monitoring or health checks.
*   **L3 - Medium:**
    *   In-house applications with no active development or support team.
    *   Components are on extended support but not due to expire within 12 months.
    *   Limited documentation or knowledge of the system.
*   **L2 - Low:**
    *   Expired vendor contract for a component.
    *   System is in-support, but the underlying technology is considered legacy or outdated.
*   **L1 - Very Low:**
    *   System is fully supported and regularly patched.

### 7.3. Impact Criteria (C)
The LLM will infer the business impact of a failure from repository documentation and context. The highest score from any single criterion determines the overall Impact Score.

*   **C6 - Catastrophic:**
    *   Direct threat to life and welfare or national security.
*   **C5 - Critical:**
    *   Major disruption to essential public services.
    *   Significant, long-term impact on public finances or the economy.
*   **C4 - High:**
    *   Loss of public trust in government services.
    *   Major data loss or breach involving sensitive citizen data.
    *   Direct financial impact exceeding £10m.
*   **C3 - Medium:**
    *   Inability to deliver a key policy objective.
    *   Significant reputational damage to the organization.
    *   Direct financial impact between £1m and £10m.
*   **C2 - Low:**
    *   Breach of statutory or legal obligations.
    *   Noticeable disruption to internal business operations.
    *   Direct financial impact under £1m.
*   **C1 - Very Low:**
    *   Minor inconvenience to staff or internal processes.

### 7.4. Risk Rating
A system is classified as 'red-rated' if its final risk score (Likelihood x Impact) is 16 or higher.


Output Specifications
- The report should be in UK English

Report Structure Template

# Code Review Report: [Repository Name]

Review Date: [Date]
Repository: [Repository Name/URL]
Reviewer: [LLM Assistant]
Application Type: [Web Application/API/Full-Stack]

- A statement to clarify to the reader that this is an AI generated report and the details should be varified by a subject matter expert. The report should be seen as a indication of potential problem areas for further investigation. 

## Executive Summary
[Max 500 words - High-level assessment with key findings]

## Critical Issues Summary
| Issue | Category | Severity | Impact | Location |
|-------|----------|----------|---------|----------|
| [Issue] | [Security/Quality/Performance] | [Critical/High/Medium/Low] | [Description] | [File:Line] |

## Legacy IT Risk Assessment Summary

### Overall Risk Rating

## Detailed Analysis

### Code Quality Assessment
#### Complexity Analysis
- Average Cyclomatic Complexity: [Score]
- Functions Exceeding Thresholds: [Count and examples]
- Code Duplication Percentage: [Percentage]

#### SOLID Principles Evaluation
| Principle | Score (1-10) | Assessment | Recommendations |
|-----------|--------------|------------|-----------------|
| Single Responsibility | [Score] | [Assessment] | [Recommendations] |
| Open-Closed | [Score] | [Assessment] | [Recommendations] |
| Liskov Substitution | [Score] | [Assessment] | [Recommendations] |
| Interface Segregation | [Score] | [Assessment] | [Recommendations] |
| Dependency Inversion | [Score] | [Assessment] | [Recommendations] |

### Testing Evaluation
- Overall Test Coverage: [Percentage]
- Line Coverage: [Percentage]
- Branch Coverage: [Percentage]
- Missing Test Areas: [List with priorities]

### Security Assessment
#### OWASP Top 10 Compliance
[Detailed findings for each applicable category]

#### Vulnerability Summary
- Critical Vulnerabilities: [Count and description]
- High Priority Issues: [Count and description]
- Medium Priority Issues: [Count and description]

### Performance Analysis
- Identified Bottlenecks: [List with impact assessment]
- Optimization Opportunities: [Specific recommendations]

### Documentation Review
- Documentation Coverage: [Assessment]
- Missing Documentation: [List with priorities]

## Recommendations and Next Steps

### Immediate Actions (Critical/High Priority)
1. [Specific recommendation with general guidance]
2. [Specific recommendation with general guidance]

### Short-term Improvements (Medium Priority)
1. [Specific recommendation with general guidance]
2. [Specific recommendation with general guidance]

### Long-term Enhancements (Low Priority)
1. [Specific recommendation with general guidance]
2. [Specific recommendation with general guidance]

## Legacy IT Risk Assessment Framework assessment
### Summary
### Likelihood Criteria (L)
### Impact Criteria (C)
### Scoring 
- **Inference and Assumptions:** For criteria that cannot be directly measured from the code (e.g., business impact, physical environment), the LLM must make educated inferences based on available information (such as README files, documentation, library age, and issue discussions).
- **Transparency:** The final report must include a dedicated section explaining all assumptions, inferences, and interpretations made during the risk assessment. Any criteria that are omitted as non-applicable must be listed with a justification.



## Severity Summary
- Critical Issues: [Count]
- High Priority Issues: [Count]
- Medium Priority Issues: [Count]
- Low Priority Issues: [Count]

## Appendices
### A. Code Examples
[Specific code snippets demonstrating issues and improvements]

### B. Metrics Summary
[Detailed metrics tables and charts if applicable]

Severity Definitions

- Critical: Security vulnerabilities, system-breaking issues, data corruption risks
- High: Performance bottlenecks affecting user experience, major code quality issues, significant maintainability problems
- Medium: Moderate maintainability concerns, minor security issues, code style inconsistencies
- Low: Documentation gaps, minor style improvements, non-critical optimizations

File Naming Convention
- Format: CodeReview_[RepoName]_[YYYY-MM-DD].md
- Include metadata header with generation timestamp and review parameters

Quality Assurance Guidelines (Optional)

Self-Validation Checklist
Before finalizing the report, verify:
- All required sections are completed with specific findings
- Severity ratings align with defined criteria
- Recommendations include actionable guidance
- Code examples are syntactically correct
- External links are accessible and relevant
- Report follows the specified markdown template
- Grammar and spelling have been validated

Cross-Reference Validation
- Issues mentioned in Executive Summary appear in detailed sections
- Severity counts in summary match detailed findings
- Recommendations address identified issues
- OWASP categories are relevant to the application type

Success Metrics

The code review should enable the development team to:
- Identify and prioritize technical debt and security vulnerabilities
- Understand specific areas requiring immediate attention
- Plan improvement initiatives based on clear recommendations
- Measure progress through quantifiable metrics and thresholds

Note: This specification is designed for web applications and APIs. Focus analysis on relevant technologies, frameworks, and patterns commonly used in modern web development including but not limited to: React, Vue, Angular, Node.js, Express, Django, Flask, REST APIs, GraphQL, and associated tooling.
```


